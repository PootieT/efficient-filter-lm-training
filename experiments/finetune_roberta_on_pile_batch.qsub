#!/bin/bash -l

# Set SCC project
#$ -P ds563

# Specify hard time limit for the job.
#   The job will be aborted if it runs longer than this time.
#   The default time is 12 hours
#$ -l h_rt=24:00:00

# Send an email when the job finishes or if it is aborted (by default no email is sent).
######$ -m ea

# Give job a name
#$ -N 'finetune_pile_batch'

# Combine output and error files into a single file
#$ -j y

# request 6 cores, each with 6 GB RAM at least
#$ -pe omp 10
#$ -l mem_per_core=6G

# request 0 GPU
#$ -l gpus=1
#$ -l gpu_c=6.0

# Submit an array job with 5 tasks
#$ -t 1-18

# Keep track of information related to the current job
echo "=========================================================="
echo "Start date : $(date)"
echo "Job name : $JOB_NAME"
echo "Job ID : $JOB_ID  $SGE_TASK_ID"
echo "=========================================================="
nvidia-smi

module load anaconda3/5.2.0
source activate /projectnb/llamagrp/peter/CS543-final-project/envs

# huggingface related cache directory
export TRANSFORMERS_CACHE=/projectnb2/llamagrp/peter/huggingface_cache/
export HF_DATASETS_CACHE="/projectnb2/llamagrp/peter/huggingface_cache/"
# Weights and biases related environment variables, offline to disable sync
export WANDB_CONFIG_DIR="/project/llamagrp/peter"

index=$(($SGE_TASK_ID-1))
#index=32

ROOT="/projectnb2/llamagrp/peter/CS543-final-project"

TRAIN_FILES=(\
$ROOT/data/CMS100_CS100_TL0.03_TH0.99_PH0.01/filtered_data.json \
$ROOT/data/CMS100_CS100_TL0.03_TH0.99_PH0.01/filtered_data.json \
$ROOT/data/CMS100_CS100_TL0.03_TH0.99_PH0.01/filtered_data.json \
$ROOT/data/CMS100_CS100_TL0.1_TH0.99_PH0.1/filtered_data.json \
$ROOT/data/CMS100_CS100_TL0.1_TH0.99_PH0.1/filtered_data.json \
$ROOT/data/CMS100_CS100_TL0.1_TH0.99_PH0.1/filtered_data.json \
$ROOT/data/CMS100_CS1000_TL0.01_TH0.99_PH1.0/filtered_data.json \
$ROOT/data/CMS100_CS1000_TL0.01_TH0.99_PH1.0/filtered_data.json \
$ROOT/data/CMS100_CS1000_TL0.01_TH0.99_PH1.0/filtered_data.json \
$ROOT/data/CMS100_CS1000_TL0.1_TH0.99_PH0.01/filtered_data.json \
$ROOT/data/CMS100_CS1000_TL0.1_TH0.99_PH0.01/filtered_data.json \
$ROOT/data/CMS100_CS1000_TL0.1_TH0.99_PH0.01/filtered_data.json \
$ROOT/data/CMS100_CS1000_TL0.1_TH0.99_PH0.1/filtered_data.json \
$ROOT/data/CMS100_CS1000_TL0.1_TH0.99_PH0.1/filtered_data.json \
$ROOT/data/CMS100_CS1000_TL0.1_TH0.99_PH0.1/filtered_data.json \
$ROOT/data/CMS100_CS3000_TL0.01_TH0.99_PH1.0/filtered_data.json \
$ROOT/data/CMS100_CS3000_TL0.01_TH0.99_PH1.0/filtered_data.json \
$ROOT/data/CMS100_CS3000_TL0.01_TH0.99_PH1.0/filtered_data.json \
)
TRAIN_FILE=${TRAIN_FILES[$index]}

SEEDS=(\
42 43 44 \
42 43 44 \
42 43 44 \
42 43 44 \
42 43 44 \
42 43 44 \
)
SEED=${SEEDS[$index]}

echo "Train file="$TRAIN_FILE" seed="$SEED
#python run_mlm.py \
#    --model_name_or_path roberta-base \
#    --dataset_name the_pile \
#    --train_file $TRAIN_FILE \
#    --streaming true \
#    --max_seq_length 512 \
#    --max_eval_samples 1000 \
#    --run_name $RUN_NAME \
#    --output_dir "${ROOT}/dump/${RUN_NAME}" \
#    --per_device_train_batch_size 16 \
#    --gradient_accumulation_steps 4 \
#    --per_device_eval_batch_size 8 \
#    --learning_rate 5e-5 \
#    --do_train \
#    --do_eval \
#    --evaluation_strategy steps \
#    --eval_steps 200 \
#    --logging_steps 500 \
#    --max_steps 75000 \
#    --seed $SEED \
#    --overwrite_output_dir true \
#    --fp16 true
