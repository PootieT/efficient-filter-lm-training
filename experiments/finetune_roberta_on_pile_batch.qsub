#!/bin/bash -l

# Set SCC project
#$ -P ds563

# Specify hard time limit for the job.
#   The job will be aborted if it runs longer than this time.
#   The default time is 12 hours
#$ -l h_rt=24:00:00

# Send an email when the job finishes or if it is aborted (by default no email is sent).
######$ -m ea

# Give job a name
#$ -N 'FT-batch'

# Combine output and error files into a single file
#$ -j y
#$ -o log

# request 6 cores, each with 6 GB RAM at least
#$ -pe omp 8
#$ -l mem_per_core=6G

# request 0 GPU
#$ -l gpus=1
#$ -l gpu_c=6.5

# Submit an array job with 5 tasks
#$ -t 3-18

# Keep track of information related to the current job
echo "=========================================================="
echo "Start date : $(date)"
echo "Job name : $JOB_NAME"
echo "Job ID : $JOB_ID  $SGE_TASK_ID"
echo "=========================================================="
nvidia-smi

module load anaconda3/5.2.0
source activate /projectnb/llamagrp/peter/CS543-final-project/envs

# huggingface related cache directory
export TRANSFORMERS_CACHE=/projectnb2/llamagrp/peter/huggingface_cache/
export HF_DATASETS_CACHE="/projectnb2/llamagrp/peter/huggingface_cache/"
# Weights and biases related environment variables, offline to disable sync
export WANDB_CONFIG_DIR="/project/llamagrp/peter"

index=$(($SGE_TASK_ID-1))
#index=0

ROOT="/projectnb2/llamagrp/peter/CS543-final-project"

RUN_NAMES=(\
CMS100_CS100_TL0.03_TH0.99_PH0.01 \
CMS100_CS100_TL0.03_TH0.99_PH0.01 \
CMS100_CS100_TL0.03_TH0.99_PH0.01 \
CMS100_CS100_TL0.1_TH0.99_PH0.1 \
CMS100_CS100_TL0.1_TH0.99_PH0.1 \
CMS100_CS100_TL0.1_TH0.99_PH0.1 \
CMS100_CS1000_TL0.01_TH0.99_PH1.0 \
CMS100_CS1000_TL0.01_TH0.99_PH1.0 \
CMS100_CS1000_TL0.01_TH0.99_PH1.0 \
CMS100_CS1000_TL0.1_TH0.99_PH0.01 \
CMS100_CS1000_TL0.1_TH0.99_PH0.01 \
CMS100_CS1000_TL0.1_TH0.99_PH0.01 \
CMS100_CS1000_TL0.1_TH0.99_PH0.1 \
CMS100_CS1000_TL0.1_TH0.99_PH0.1 \
CMS100_CS1000_TL0.1_TH0.99_PH0.1 \
CMS100_CS3000_TL0.01_TH0.99_PH1.0 \
CMS100_CS3000_TL0.01_TH0.99_PH1.0 \
CMS100_CS3000_TL0.01_TH0.99_PH1.0 \
)
RUN_NAME=${RUN_NAMES[$index]}
TRAIN_FILE=$ROOT/data/$RUN_NAME/filtered_data.json

SEEDS=(\
42 43 44 \
42 43 44 \
42 43 44 \
42 43 44 \
42 43 44 \
42 43 44 \
)
SEED=${SEEDS[$index]}

echo "Train file="$TRAIN_FILE" seed="$SEED
python evaluation/run_mlm.py \
    --model_name_or_path roberta-base \
    --dataset_name the_pile \
    --train_file $TRAIN_FILE \
    --validation_file $ROOT/data/the_pile_valid_1000.json \
    --streaming true \
    --max_seq_length 512 \
    --max_eval_samples 1000 \
    --run_name "${RUN_NAME}_${SEED}" \
    --output_dir "${ROOT}/dump/${RUN_NAME}_${SEED}" \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 4 \
    --per_device_eval_batch_size 8 \
    --learning_rate 5e-5 \
    --do_train \
    --do_eval \
    --evaluation_strategy steps \
    --eval_steps 200 \
    --logging_steps 500 \
    --max_steps 75000 \
    --seed $SEED \
    --overwrite_output_dir true \
    --save_total_limit 2 \
    --fp16 true

echo "DONE TRAINING!"
